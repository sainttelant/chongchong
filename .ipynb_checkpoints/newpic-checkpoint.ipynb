{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_MaxPage():\n",
    "    all_url = 'http://www.mzitu.com'\n",
    "    start_html = requests.get(all_url,headers = header)\n",
    "    #找寻最大页数\n",
    "    soup = BeautifulSoup(start_html.text,\"html.parser\")\n",
    "    page = soup.find_all('a',class_='page-numbers')\n",
    "    max_page = page[-2].text\n",
    "    return max_page\n",
    "\n",
    "\n",
    "# \n",
    "def Download(href,header,title,path):\n",
    "    html = requests.get(href,headers = header)\n",
    "    soup = BeautifulSoup(html.text,'html.parser')\n",
    "    pic_max = soup.find_all('span')\n",
    "    pic_max = pic_max[10].text  # 最大页数\n",
    "    if(os.path.exists(path+title.strip().replace('?','')) and len(os.listdir(path+title.strip().replace('?',''))) >= int(pic_max)):\n",
    "        print('已完毕，跳过'+title)\n",
    "        return 1\n",
    "    print(\"开始扒取：\" + title)\n",
    "    os.makedirs(path+title.strip().replace('?',''))\n",
    "    os.chdir(path + title.strip().replace('?',''))\n",
    "    for num in range(1,int(pic_max)+1):\n",
    "        pic = href+'/'+str(num)\n",
    "        #print(pic)\n",
    "        html = requests.get(pic,headers = header)\n",
    "        mess = BeautifulSoup(html.text,\"html.parser\")\n",
    "        pic_url = mess.find('img',alt = title)\n",
    "        html = requests.get(pic_url['src'],headers = header)\n",
    "        file_name = pic_url['src'].split(r'/')[-1]\n",
    "        f = open(file_name,'wb')\n",
    "        f.write(html.content)\n",
    "        f.close()\n",
    "    print('完成'+title)\n",
    "\n",
    "def download(href,header,title):\n",
    "\n",
    "    html = requests.get(href,headers = header)\n",
    "    soup = BeautifulSoup(html.text,'html.parser')\n",
    "    pic_max = soup.find_all('span')\n",
    "    #for j in pic_max:\n",
    "        #print(j.text)\n",
    "    #print(len(pic_max))\n",
    "    pic_max = pic_max[10].text  # 最大页数\n",
    "    print(pic_max)\n",
    "\n",
    "\n",
    "'''\n",
    "#安卓端需要此语句\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "'''\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    if (os.name == 'nt'):\n",
    "        print(u'你正在使用win平台')\n",
    "    else:\n",
    "        print(u'你正在使用linux平台')\n",
    "\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/6.1.2107.204 Safari/537.36'}\n",
    "    # http请求头\n",
    "    path = 'mzitu/'\n",
    "    max_page = find_MaxPage()\n",
    "    same_url = 'http://www.mzitu.com/page/'\n",
    "\n",
    "    #线程池中线程数\n",
    "    #pool = Pool(5)\n",
    "    for n in range(1,int(max_page)+1):\n",
    "        each_url = same_url+str(n)\n",
    "        start_html = requests.get(each_url, headers=header)\n",
    "        soup = BeautifulSoup(start_html.text, \"html.parser\")\n",
    "        all_a = soup.find('div', class_='postlist').find_all('a', target='_blank')\n",
    "        for a in all_a:\n",
    "            title = a.get_text()  # 提取文本\n",
    "            if (title != ''):\n",
    "                href = a['href']\n",
    "                #print(href)  #到这一步是打开每一个需要爬虫图片的主页面 。。 eg：http://www.mzitu.com/140818\n",
    "                Download(href,header,title,path)\n",
    "                #pool.apply_async(Download,args=(href,header,title,path))\n",
    "    #pool.close()\n",
    "    #pool.join()\n",
    "    #print('所有图片已下完')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
